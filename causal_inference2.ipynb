{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNCfooUqvY63UOMIA/gm9DF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amfei/Statistical_Inference/blob/main/causal_inference2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Effect of a featute with 2 categories(yes /no)  on Previous Claims\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.stats import ttest_ind\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def causal_effect_estimation_SmokingStatus(df, treatment_col, outcome_col):\n",
        "    # Drop rows with missing values in relevant columns\n",
        "    df_clean = df.dropna().copy()\n",
        "\n",
        "\n",
        "    # Clean string and map 'Yes'/'No' to 1/0 if needed\n",
        "    if df_clean[treatment_col].dtype == object:\n",
        "        df_clean[treatment_col] = df_clean[treatment_col].astype(str).str.strip()\n",
        "        df_clean[treatment_col] = df_clean[treatment_col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "    # Ensure binary treatment column\n",
        "    if df_clean[treatment_col].nunique() != 2:\n",
        "        raise ValueError(\"This function only supports binary treatment columns (e.g., 'Yes'/'No').\")\n",
        "\n",
        "    # One-hot encode object columns (except treatment and outcome)\n",
        "    categorical_cols = df_clean.select_dtypes(include='object').columns.difference([treatment_col, outcome_col])\n",
        "    df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    # Convert boolean to int\n",
        "    bool_cols = df_encoded.select_dtypes(include='bool').columns\n",
        "    df_encoded[bool_cols] = df_encoded[bool_cols].astype(int)\n",
        "\n",
        "    # Define confounders\n",
        "    confounders = df_encoded.columns.difference([treatment_col, outcome_col])\n",
        "    X = df_encoded[confounders]\n",
        "    T = df_encoded[treatment_col]\n",
        "\n",
        "    # Standardize for logistic regression stability\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Propensity score model\n",
        "    ps_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "    ps_model.fit(X_scaled, T)\n",
        "    df_encoded['propensity_score'] = ps_model.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "    # Split into treated and control groups\n",
        "    treated = df_encoded[df_encoded[treatment_col] == 1]\n",
        "    control = df_encoded[df_encoded[treatment_col] == 0]\n",
        "\n",
        "    # Nearest neighbor matching on propensity score\n",
        "    nn = NearestNeighbors(n_neighbors=1)\n",
        "    nn.fit(control[['propensity_score']])\n",
        "    distances, indices = nn.kneighbors(treated[['propensity_score']])\n",
        "    matched_control = control.iloc[indices.flatten()].reset_index(drop=True)\n",
        "    matched_treated = treated.reset_index(drop=True)\n",
        "\n",
        "    # Extract outcomes\n",
        "    treated_outcomes = matched_treated[outcome_col]\n",
        "    control_outcomes = matched_control[outcome_col]\n",
        "\n",
        "    # Estimate ATE and other statistics\n",
        "    ate = treated_outcomes.mean() - control_outcomes.mean()\n",
        "    t_stat, p_val = ttest_ind(treated_outcomes, control_outcomes)\n",
        "\n",
        "    # Cohen's d\n",
        "    s_pooled = np.sqrt(\n",
        "        ((len(treated_outcomes) - 1) * treated_outcomes.var() +\n",
        "         (len(control_outcomes) - 1) * control_outcomes.var()) /\n",
        "        (len(treated_outcomes) + len(control_outcomes) - 2)\n",
        "    )\n",
        "    cohens_d = ate / s_pooled\n",
        "\n",
        "    # Required sample size for 80% power\n",
        "    analysis = TTestIndPower()\n",
        "    required_n = analysis.solve_power(effect_size=abs(cohens_d), alpha=0.05, power=0.8, ratio=1)\n",
        "\n",
        "    # Return summary as DataFrame\n",
        "    summary_results = pd.DataFrame({\n",
        "        \"Metric\": [\"ATE\", \"T-Statistic\", \"P-Value\", \"Cohen's d\", \"Required Sample Size\"],\n",
        "        \"Value\": [ate, t_stat, p_val, cohens_d, required_n]\n",
        "    })\n",
        "\n",
        "    return summary_results\n",
        "\n",
        "causal_effect_estimation_SmokingStatus(df, treatment_col='Smoking Status', outcome_col='Previous Claims')"
      ],
      "metadata": {
        "id": "5hnmjTZb8xXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['Age', 'Gender', 'Annual Income', 'Marital Status',\n",
        "       'Number of Dependents', 'Education Level', 'Occupation', 'Health Score',\n",
        "       'Location', 'Policy Type', 'Previous Claims', 'Vehicle Age',\n",
        "       'Credit Score', 'Insurance Duration', 'Premium Amount',\n",
        "       'Policy Start Date', 'Customer Feedback', 'Smoking Status',\n",
        "       'Exercise Frequency', 'Property Type']"
      ],
      "metadata": {
        "id": "EDGLhCwy2A-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Effect of multi categories(>2) featutes on Previous Claims\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.stats import ttest_ind\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv(\"InsuranceDataset.csv\")\n",
        "\n",
        "df.drop('Policy Start Date', axis = 1, inplace = True)\n",
        "\n",
        "def causal_effect_estimation(df, treatment_col, outcome_col):\n",
        "    # Drop rows with missing values in relevant columns\n",
        "    df_clean = df.dropna()\n",
        "\n",
        "    # If binary Yes/No, map to 1/0\n",
        "    if df_clean[treatment_col].dtype == object:\n",
        "        unique_vals = df_clean[treatment_col].dropna().unique()\n",
        "        if set(unique_vals) == {'Yes', 'No'}:\n",
        "            df_clean.loc[:, treatment_col] = df_clean[treatment_col].map({'Yes': 1, 'No': 0})\n",
        "        else:\n",
        "            # Automatically factorize any non-binary, non-Yes/No column\n",
        "            df_clean[treatment_col], _ = pd.factorize(df_clean[treatment_col])\n",
        "\n",
        "    # One-hot encode remaining object columns (excluding treatment and outcome)\n",
        "    categorical_cols = df_clean.select_dtypes(include='object').columns.difference([treatment_col, outcome_col])\n",
        "    df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    # Convert boolean to int\n",
        "    df_encoded[df_encoded.select_dtypes(include='bool').columns] = df_encoded.select_dtypes(include='bool').astype(int)\n",
        "\n",
        "    # Define confounders\n",
        "    confounders = df_encoded.columns.difference([treatment_col, outcome_col])\n",
        "\n",
        "    # Propensity Score Estimation\n",
        "    X_ps = df_encoded[confounders]\n",
        "    T = df_encoded[treatment_col]\n",
        "\n",
        "    ps_model = LogisticRegression(solver='lbfgs', max_iter=500)\n",
        "    ps_model.fit(X_ps, T)\n",
        "\n",
        "    # If binary, select prob for class 1; else take max prob\n",
        "    proba = ps_model.predict_proba(X_ps)\n",
        "    if proba.shape[1] == 2:\n",
        "        df_encoded['propensity_score'] = proba[:, 1]\n",
        "    else:\n",
        "        df_encoded['propensity_score'] = proba.max(axis=1)\n",
        "\n",
        "    # Define treated and control for one-vs-all\n",
        "    treated = df_encoded[df_encoded[treatment_col] == df_encoded[treatment_col].unique()[0]]\n",
        "    control = df_encoded[df_encoded[treatment_col] != df_encoded[treatment_col].unique()[0]]\n",
        "\n",
        "    # Nearest neighbor matching\n",
        "    nn = NearestNeighbors(n_neighbors=1)\n",
        "    nn.fit(control[['propensity_score']])\n",
        "    distances, indices = nn.kneighbors(treated[['propensity_score']])\n",
        "    matched_control = control.iloc[indices.flatten()].reset_index(drop=True)\n",
        "    matched_treated = treated.reset_index(drop=True)\n",
        "\n",
        "    # Extract outcomes\n",
        "    treated_outcomes = matched_treated[outcome_col]\n",
        "    control_outcomes = matched_control[outcome_col]\n",
        "\n",
        "    # ATE\n",
        "    ate = treated_outcomes.mean() - control_outcomes.mean()\n",
        "\n",
        "    # T-test\n",
        "    t_stat, p_val = ttest_ind(treated_outcomes, control_outcomes)\n",
        "\n",
        "    # Cohen's d\n",
        "    s_pooled = np.sqrt(((len(treated_outcomes) - 1) * treated_outcomes.var() +\n",
        "                        (len(control_outcomes) - 1) * control_outcomes.var()) /\n",
        "                        (len(treated_outcomes) + len(control_outcomes) - 2))\n",
        "    cohens_d = ate / s_pooled\n",
        "\n",
        "    # Required Sample Size\n",
        "    analysis = TTestIndPower()\n",
        "    required_n = analysis.solve_power(effect_size=abs(cohens_d), alpha=0.05, power=0.8, ratio=1)\n",
        "\n",
        "    # Result summary\n",
        "    summary_results = pd.DataFrame({\n",
        "        \"Metric\": [\"ATE\", \"T-Statistic\", \"P-Value\", \"Cohen's d\", \"Required Sample Size\"],\n",
        "        \"Value\": [ate, t_stat, p_val, cohens_d, required_n]\n",
        "    })\n",
        "\n",
        "    return summary_results\n",
        "\n",
        "causal_effect_estimation(df, treatment_col='Property Type', outcome_col='Previous Claims')"
      ],
      "metadata": {
        "id": "uY3mQUvu2BBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}