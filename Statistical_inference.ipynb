{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPtPI3r/eukYzC0xR9IT27j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amfei/Causal_Inference/blob/main/Statistical_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Problem: Estimating the Effect of a **Loyalty** program on Insurance Claims.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Suppose an insurance company wants to estimate the effect of enrolling customers in a loyalty program on the number of insurance claims they file.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### There are various methods for **causal inference** (process of determining whether one variable, often called the \"treatment\" or \"cause\", directly affects another variable, the \"outcome\" or \"effect\"), including **randomized controlled trials (RCTs)**, **instrumental variables (IV), propensity score matching**, and more recently, methods using g**raphical models and machine learning.**\n",
        "\n",
        "This is different from **correlation**, which simply measures whether two variables **move together**, without implying that one causes the other.\n",
        "\n",
        "\n",
        "Since customers are not randomly assigned to the loyalty program, there may be **confounding variables** that influence both the enrollment and the outcome (e.g., age, income, past claims history).\n",
        "\n",
        "**Steps for Propensity Score Application**\n",
        "\n",
        "**1. Data Preparation:** Create a dataset with treatment, outcome, and covariates.\n",
        "\n",
        "\n",
        "**2. Propensity Score Estimation:** Estimate the probability of treatment assignment based on observed covariates.\n",
        "\n",
        "*   Propensity Score is probability of a unit (e.g., a customer) receiving the treatment given their observed characteristics.\n",
        "\n",
        "\n",
        "**3. Matching/Weighting/Stratification:** Use propensity scores to balance the treatment and control groups.\n",
        "\n",
        "**4. Outcome Analysis:** Compare the outcomes between the balanced groups to estimate the treatment effect.\n",
        "\n"
      ],
      "metadata": {
        "id": "av8ksMsDGCE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "data = {\n",
        "    'age': np.random.randint(18, 70, size=num_samples),\n",
        "    'income': np.random.normal(50000, 10000, size=num_samples),\n",
        "    'past_claims': np.random.randint(0, 5, size=num_samples),\n",
        "    'loyalty_program': np.random.binomial(1, 0.3, size=num_samples),  # 30% in loyalty program\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce a causal relationship\n",
        "df['num_claims'] = df['loyalty_program'] * -0.5 + df['past_claims'] * 0.3 + np.random.normal(0, 1, num_samples)\n",
        "df['num_claims'] = round(df['num_claims'].apply(lambda x: max(x, 0)))  # Ensure non-negative claims\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "my-844AEER8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Matching:** Treated units (enrolled in the loyalty program) are matched with control units (not enrolled) based on their propensity scores using nearest neighbor matching.\n",
        "\n",
        "#### Matching is a method to pair each treated unit with one or more control units that have similar propensity scores. This aims to balance the distribution of observed covariates between the treated and control groups, reducing the bias due to confounding variables.\n",
        "\n",
        "#### Nearest Neighbor Matching: Each treated unit is matched with the closest control unit(s) based on the propensity score. In this example, we use 1-to-1 nearest neighbor matching.\n",
        "\n",
        "#### In observational studies, like our insurance, the treatment (enrollment in a loyalty program) is not randomly assigned. This can lead to biased estimates of the treatment effect because the treated and untreated groups might differ in ways that also affect the outcome (number of claims). Matching helps to make these groups comparable.\n",
        "\n",
        "#### How do we do Matching?\n",
        "### We use propensity scores, which are the **estimated probabilities that a customer will be in the loyalty program based on their characteristics** (age, income, past claims). We then match each treated customer with one or more control customers who have similar propensity scores.\n",
        "\n",
        "## Steps in Matching :\n",
        "\n",
        "**Estimate Propensity Scores:** Use logistic regression to calculate the probability of each customer being in the loyalty program based on their age, income, and past claims.\n",
        "\n",
        "**Find Matches:** For each customer in the loyalty program (treated), find a customer not in the program (control) with a similar propensity score. This is like finding \"twins\" based on the probability of receiving the treatment.\n",
        "\n",
        "**Create Matched Dataset:** Combine the treated customers and their matched controls into a new dataset that will be used for analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "rXwt8UmVLFNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define features and treatment\n",
        "X = df[['age', 'income', 'past_claims']]\n",
        "y = df['loyalty_program']\n",
        "\n",
        "# Fit logistic regression to estimate propensity scores\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(X, y)\n",
        "\n",
        "# Predict propensity scores\n",
        "df['propensity_score'] = logistic.predict_proba(X)[:, 1]\n",
        "\n",
        "# Display the first few rows with propensity scores\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "5_BrSZhq62m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Separate treated and control groups\n",
        "treated = df[df['loyalty_program'] == 1]\n",
        "control = df[df['loyalty_program'] == 0]\n",
        "\n",
        "# Fit nearest neighbors model\n",
        "nn = NearestNeighbors(n_neighbors=1)\n",
        "nn.fit(control[['propensity_score']])\n",
        "\n",
        "# Find nearest neighbors in the control group for each treated unit\n",
        "distances, indices = nn.kneighbors(treated[['propensity_score']])\n",
        "\n",
        "# Get matched control units\n",
        "matched_controls = control.iloc[indices.flatten()]\n",
        "\n",
        "# Combine treated and matched control units\n",
        "matched_df = pd.concat([treated, matched_controls])\n",
        "\n",
        "# Display the first few rows of the matched dataset\n",
        "print(matched_df.head())\n"
      ],
      "metadata": {
        "id": "g3fRfsavESC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average treatment effect (ATE) measures the average effect of a treatment on an outcome variable. There are different methods to estimate the ATE but it is often estimated using sample means as :"
      ],
      "metadata": {
        "id": "dcXs8R699HxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average treatment effect\n",
        "ate = matched_df[matched_df['loyalty_program'] == 1]['num_claims'].mean() - \\\n",
        "      matched_df[matched_df['loyalty_program'] == 0]['num_claims'].mean()\n",
        "\n",
        "print(f'Average Treatment Effect (ATE): {ate}')\n"
      ],
      "metadata": {
        "id": "aTsCzVYoESEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define pre and post periods\n",
        "pre_period_data = matched_df[matched_df['loyalty_program'] == 1]['num_claims']\n",
        "post_period_data = matched_df[matched_df['loyalty_program'] == 0]['num_claims']\n",
        "\n",
        "# Calculate the average treatment effect\n",
        "ate = np.mean(post_period_data) - np.mean(pre_period_data)\n"
      ],
      "metadata": {
        "id": "oxkCoVaIXhzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(pre_period_data, bins=20, alpha=0.5, color='blue', label='Pre Period')\n",
        "plt.hist(post_period_data, bins=20, alpha=0.5, color='red', label='Post Period')\n",
        "#plt.axvline(x=np.mean(pre_period_data), color='blue', linestyle='--', label='Pre Period Mean')\n",
        "#plt.axvline(x=np.mean(post_period_data), color='red', linestyle='--', label='Post Period Mean')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Causal Inference Example')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the average treatment effect\n",
        "print(\"Average Treatment Effect (ATE):\", ate)"
      ],
      "metadata": {
        "id": "Q-MpH6azN--H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To solve the insurance loyalty claim problem we can also the **DoWhy** library, with thefollowing steps:\n",
        "\n",
        "\n",
        "Causal Model Specification: Define the causal model including treatment, outcome, and confounders.\n",
        "\n",
        "Identify the Causal Effect: Use DoWhy to identify the causal effect.\n",
        "\n",
        "Estimate the Causal Effect: Estimate the causal effect using an appropriate method.\n",
        "\n",
        "Refute the Estimate: Perform robustness checks to ensure the validity of the causal effect"
      ],
      "metadata": {
        "id": "PXlsHMN7Wau3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##!pip install dowhy"
      ],
      "metadata": {
        "id": "Hp2S1c6H-UE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import dowhy\n",
        "from dowhy import CausalModel\n",
        "\n",
        "# Define the causal model\n",
        "model = CausalModel(\n",
        "    data=df,\n",
        "    treatment='loyalty_program',\n",
        "    outcome='num_claims',\n",
        "    common_causes=['age', 'income', 'past_claims']\n",
        ")\n",
        "\n",
        "# View the model\n",
        "model.view_model()\n"
      ],
      "metadata": {
        "id": "FF9mBqSyVcUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methodology: The estimand is identified using backdoor adjustment (in the first estimand) and instrumental variables (in the second estimand), as indicated by the **backdoor** and **iv** names.\n",
        "\n",
        "\n",
        "Estimand Type: The identified estimand is for the Average Treatment Effect (ATE), which measures the average effect of the treatment on the outcome.\n",
        "\n",
        "Estimand Expression: This describes the mathematical expression used to estimate the causal effect, typically involving conditional expectations or derivatives.\n",
        "\n",
        "Estimand Assumptions: These assumptions specify the conditions under which the identified estimand is valid. For example, unconfoundedness assumes that there are no unobserved confounders affecting both the treatment assignment and the outcome.\n",
        "\n"
      ],
      "metadata": {
        "id": "cfP5zCrzDmgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identified_estimand = model.identify_effect()\n",
        "print(identified_estimand)\n"
      ],
      "metadata": {
        "id": "crq06kgsVkH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backdoor Estimand: It shows how the loyalty program might change the number of claims people make, once we think about differences in past claims, income, and age. We have to think about what else could affect both the loyalty program and the number of claims.\n",
        "\n",
        "IV and Frontdoor Estimands: These couldn't be used here because we didn't have good examples of other things that could help us understand how the loyalty program affects the number of claims people make.\n",
        "\n"
      ],
      "metadata": {
        "id": "oCw8f0IXOwqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LTXS_hHsDk_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimate = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"backdoor.linear_regression\"\n",
        ")\n",
        "\n",
        "print(estimate)\n"
      ],
      "metadata": {
        "id": "nr-E4RmGESKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The estimated ATE of joining the loyalty program on the number of claims is approximately -0.291. This suggests that, on average, joining the loyalty program is associated with a decrease in the number of claims. However, be cautious in interpreting the sign of the estimate (- or +) as it depends on how the outcome variable is coded and the direction of the effect being studied."
      ],
      "metadata": {
        "id": "NCZ5lea_PRQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Refutation**: We perform robustness checks to ensure the causal effect is not due to random chance or unobserved confounders."
      ],
      "metadata": {
        "id": "kH_YeXbidbTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refute the causal estimate\n",
        "refutation = model.refute_estimate(\n",
        "    identified_estimand,\n",
        "    estimate,\n",
        "    method_name=\"placebo_treatment_refuter\"\n",
        ")\n",
        "\n",
        "print(refutation)"
      ],
      "metadata": {
        "id": "ItYt25X2WCoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The original estimated effect was -0.2908133591203415, suggesting a negative impact of the treatment.\n",
        "\n",
        "### After applying the placebo refutation, the new estimated effect is very close to zero (0.003572994964809586), which suggests that the observed effect may be due to a placebo effect or random chance.\n",
        "\n",
        "The high p-value of 0.96 further supports that the difference between the original and refuted effects is not statistically significant.\n",
        "\n",
        "## Conclusion:\n",
        "\n",
        "This refutation suggests that the effect of the treatment is not robust against a placebo treatment. The observed effect is likely to be due to a placebo effect or random chance rather than a true causal effect of the treatment."
      ],
      "metadata": {
        "id": "y1QyiljaW8nM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the context of evaluating the effect of a loyalty program on insurance claims, statistical tests can be used to determine whether the observed differences in claims between customers who participate in the loyalty program and those who do not are statistically significant.\n",
        "\n",
        "\n",
        "\n",
        "### **Statistical Tests**\n",
        "\n",
        "**t-Test:** Used to compare the means of two groups (e.g., claims from loyalty program participants vs. non-participants).\n",
        "    \n",
        "**Chi-Square Test:** Used to compare categorical variables (e.g., frequency of claims in different groups).\n",
        "\n",
        "**ANOVA** (Analysis of Variance): Used to compare means across multiple groups.\n",
        "    \n",
        "Regression Analysis: Used to understand the relationship between dependent and independent variables (e.g., effect of loyalty program on the number of claims).\n",
        "\n",
        "\n",
        "\n",
        "**Run t-Test Experiment:**\n",
        "\n",
        "\n",
        "Collect data over a specified period.\n",
        "\n",
        "Analyze the Data:\n",
        "Calculate the means and standard deviations of claims for both groups.Ensure that the data collection is consistent and covers the necessary sample size.\n",
        "\n",
        "\n",
        "\n",
        "**Effect Size:**\n",
        "\n",
        "Effect size is a measure used to quantify the magnitude of the difference or relationship between variables in a statistical analysis\n",
        "\n",
        "ùëë= (mean control ‚àí mean treatment )/ std_dev\n",
        "\n",
        "\n",
        "**Sample Size**:\n",
        "\n",
        "Determine the sample size needed to detect the effect size with desired power (typically 0.8 or 80%) and significance level (Œ±, typically 0.05).\n",
        "\n",
        "\n",
        "ùëõ=(ùëçùõº/2 +ùëçùõΩ)^2 * 2*ùúé^2 /effect size\n",
        "\n",
        "\n",
        "Use Z-scores for the desired confidence level (e.g., Z = 1.96 for 95% confidence) and power (e.g., Z = 0.84 for 80% power).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WPwWP3CKupYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import math\n",
        "\n",
        "#\n",
        "mean_control = 100\n",
        "mean_treatment = 80\n",
        "std_dev = 20\n",
        "alpha = 0.05  # Significance level\n",
        "power = 0.8  # Desired power\n",
        "\n",
        "# Step 2: Calculate Effect Size (Cohen's d)\n",
        "effect_size = (mean_treatment - mean_control ) / std_dev\n",
        "print(f\"Effect Size (Cohen's d): {effect_size}\")\n",
        "\n",
        "# Step 3: Calculate Required Sample Size\n",
        "z_alpha = stats.norm.ppf(1 - alpha / 2)\n",
        "z_beta = stats.norm.ppf(power)\n",
        "\n",
        "sample_size = (2* (z_alpha + z_beta) ** 2  * (std_dev ** 2) )/ effect_size ** 2\n",
        "sample_size = math.ceil(sample_size)\n",
        "print(f\"Required Sample Size per Group: {sample_size}\")\n",
        "\n",
        "# Calculate Statistical Power\n",
        "# Using observed effect size and recalculating Z value\n",
        "power_analysis = TTestIndPower()\n",
        "\n",
        "power = power_analysis.solve_power(effect_size=effect_size, nobs1=sample_size, alpha=alpha, ratio=1.0, alternative='two-sided')\n",
        "\n",
        "print(\"Statistical Power:\", power) # the probability that a statistical test will correctly reject the null hypothesis when it is false"
      ],
      "metadata": {
        "id": "eORx7Uc-cTtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Simulate Data Collection\n",
        "np.random.seed(42)\n",
        "control_group = np.random.normal(mean_control, std_dev, sample_size)\n",
        "treatment_group = np.random.normal(mean_treatment, std_dev, sample_size)\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'group': ['control'] * sample_size + ['treatment'] * sample_size,\n",
        "    'claims': np.concatenate([control_group, treatment_group])\n",
        "})\n",
        "data"
      ],
      "metadata": {
        "id": "a_ZKivpRX1fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Implement the Experiment:**\n",
        "\n",
        "Perform a hypothesis test (e.g., t-test) to compare the means of the two groups.\n",
        "\n",
        "Ensure randomization to avoid bias.\n",
        "\n",
        "**Hypothesis**: \"The new loyalty program reduces the average number of insurance claims.\"\n",
        "\n",
        "**Null hypothesis (ùêª0)**: The loyalty program has no effect on the number of claims.\n",
        "\n",
        "**Alternative hypothesis (ùêª1)**: The loyalty program reduces the number of claims.\n",
        "\n",
        "\n",
        "**p-value**: The probability of obtaining the observed results if the null hypothesis is true. A low p-value (typically < 0.05) indicates that the null hypothesis can be rejected.\n",
        "\n"
      ],
      "metadata": {
        "id": "TR9VDUS8r8_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform A/B test\n",
        "control_group = data[data['group'] == 'control']['claims']\n",
        "treatment_group = data[data['group'] == 'treatment']['claims']\n",
        "\n",
        "t_stat, p_value = stats.ttest_ind(control_group, treatment_group)\n",
        "\n",
        "print(f'T-statistic: {t_stat}, P-value: {p_value}')\n",
        "\n",
        "# Plotting the distributions\n",
        "plt.hist(control_group, alpha=0.5, label='Control Group')\n",
        "plt.hist(treatment_group, alpha=0.5, label='Treatment Group')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Number of Claims')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Claims in Control and Treatment Groups')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1z7LCwoconF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Determine the Number of Days Required:**\n",
        "\n",
        "Estimate how long you need to run the test to collect the required sample size.\n",
        "\n",
        "\n",
        "days = sample size /(daily visitors √ó proportion eligible)\n",
        "\n"
      ],
      "metadata": {
        "id": "zO3K5S_JryL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Determine Number of Days Required\n",
        "daily_claim = 200\n",
        "proportion_eligible = 0.5  # Assuming equal split\n",
        "\n",
        "days_required = sample_size / (daily_claim * proportion_eligible)\n",
        "print(f\"Number of Days Required to Run the Test: {math.ceil(days_required)} days\")\n"
      ],
      "metadata": {
        "id": "3MZxhY2Qcovt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence interval for the mean difference\n",
        "mean_diff = np.mean(control_group) - np.mean(treatment_group)\n",
        "se_diff = np.sqrt((np.var(control_group)/sample_size) + (np.var(treatment_group)/sample_size))\n",
        "confidence_interval_diff = stats.norm.interval(1-alpha, loc=mean_diff, scale=se_diff)\n",
        "print(f\"95% Confidence Interval for the Mean Difference: {confidence_interval_diff}\")"
      ],
      "metadata": {
        "id": "ur9fPHt_1Gu9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}