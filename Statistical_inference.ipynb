{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPN8P9OzPqOO56CZ+PYmnK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amfei/Statistical_Inference/blob/main/Statistical_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate the causal effect of a loyalty program on the number of insurance claims.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Step 1: Generate Synthetic Data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Covariates (Confounders)\n",
        "age = np.random.randint(25, 65, n_samples)  # More realistic working-age range\n",
        "income = np.random.randint(30000, 90000, n_samples)  # More balanced distribution\n",
        "past_claims = np.random.poisson(1.2, n_samples)  # Slightly lower past claims on average\n",
        "\n",
        "# Adjusted Treatment Variable: Loyalty Program Enrollment\n",
        "# Ensuring more realistic overlap in propensity scores\n",
        "logit_val = -0.015 * (income - 60000) - 0.025 * (age - 45) + 0.15 * past_claims\n",
        "logit_val = np.clip(logit_val, -4, 4)  # Prevent extreme values to ensure overlap\n",
        "loyalty_program = np.random.binomial(1, p=1 / (1 + np.exp(-logit_val)), size=n_samples)\n",
        "\n",
        "# Outcome Variable: Number of Insurance Claims (affected by loyalty program)\n",
        "num_claims = np.random.poisson(1 + 0.1 * past_claims - 0.4 * loyalty_program, n_samples)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'age': age,\n",
        "    'income': income,\n",
        "    'past_claims': past_claims,\n",
        "    'loyalty_program': loyalty_program,\n",
        "    'num_claims': num_claims\n",
        "})\n",
        "\n",
        "# Plot distribution of claims before matching\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.kdeplot(df[df['loyalty_program'] == 1]['num_claims'], label='Treated (Loyalty Program)', fill=True)\n",
        "sns.kdeplot(df[df['loyalty_program'] == 0]['num_claims'], label='Control (No Loyalty Program)', fill=True)\n",
        "plt.xlabel(\"Number of Claims\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Distribution of Claims Before Matching\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "df"
      ],
      "metadata": {
        "id": "mzP6YrwDz6sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Step 2: Estimate Propensity Scores : Uses logistic regression to predict the probability of enrollment.\n",
        "\n",
        "X = df[['age', 'income', 'past_claims']]\n",
        "y = df['loyalty_program']\n",
        "\n",
        "\n",
        "# Standardizing Features to Improve Logistic Regression Performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Logistic Regression Model for Propensity Score Estimation\n",
        "propensity_model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "propensity_model.fit(X_scaled, y)\n",
        "\n",
        "# Compute Propensity Scores (Probability of Enrolling)\n",
        "df['propensity_score'] = propensity_model.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "# EXPECTED OUTCOME: Each customer gets an estimated probability of enrolling in the loyalty program.\n",
        "\n",
        "# INTERPRETATION: Customers with similar propensity scores have similar characteristics, allowing fair comparison.\n",
        "\n",
        "\n",
        "# Plot propensity score distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df[df['loyalty_program'] == 1]['propensity_score'], label=\"Treated\", kde=True, color=\"blue\", alpha=0.5)\n",
        "sns.histplot(df[df['loyalty_program'] == 0]['propensity_score'], label=\"Control\", kde=True, color=\"red\", alpha=0.5)\n",
        "plt.xlabel(\"Propensity Score\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Propensity Score Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "df"
      ],
      "metadata": {
        "id": "7LsYClyaCHXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With **matching**, we ensure that the enrolled and non-enrolled groups have **similar distributions of income, age, and past claims**.\n",
        "\n",
        "#This means that any remaining difference in claims must be due to the **loyalty program** itself, since other confounding effects are balanced.\n"
      ],
      "metadata": {
        "id": "B_BLlueg-1Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Matching (Finding Similar Customers)\n",
        "treated = df[df['loyalty_program'] == 1]  # Customers enrolled in the loyalty program\n",
        "control = df[df['loyalty_program'] == 0]  # Customers not enrolled\n",
        "\n",
        "# Nearest Neighbors Matching using Propensity Scores: Matching balances confounding variables, making the comparison fair.\n",
        "# Matching ensures we only compare statistically similar customers, eliminating bias.\n",
        "# Find similar untreated customers (control group):\n",
        "\n",
        "# For each treated customer (loyalty_program = 1), we find the nearest untreated customer (loyalty_program = 0) with a similar propensity score.\n",
        "# This is done using Nearest Neighbors Matching, which finds the control group customer whose propensity score is closest to a treated customer.\n",
        "# The control customer is selected without replacement to ensure independence.\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=1)\n",
        "nn.fit(control[['propensity_score']])\n",
        "distances, indices = nn.kneighbors(treated[['propensity_score']])  # Find closest control\n",
        "\n",
        "# Matched Control Group\n",
        "matched_control = control.iloc[indices.flatten()].reset_index(drop=True)\n",
        "\n",
        "# Create a Balanced Matched Dataset\n",
        "matched_df = pd.concat([treated.reset_index(drop=True), matched_control], axis=0)\n",
        "\n",
        "# We paired each treated customer with their closest control.\n",
        "# Now, our treated and control groups should have similar distributions of age, income, and past claims.\n",
        "\n",
        "\n",
        "# EXPECTED OUTCOME: A control group similar to the treated group in terms of age, income, and past claims.\n",
        "\n",
        "# INTERPRETATION: Now, the only key difference between the two groups should be whether they enrolled or not.\n",
        "matched_df"
      ],
      "metadata": {
        "id": "CFpsdmFNMwwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once we have a balanced control group, we can compare the number of claims between the two groups.\n",
        "\n",
        "# Comparing Outcomes\n",
        "# The treated group consists of customers who enrolled in the loyalty program.\n",
        "# The control group consists of customers who didn‚Äôt enroll, but are otherwise very similar.\n",
        "# We compare the average number of claims in both groups.\n",
        "\n",
        "# Step 4: Outcome Analysis - Compare Claims in Treated vs. Matched Control Group\n",
        "treated_claims = matched_df[matched_df['loyalty_program'] == 1]['num_claims']\n",
        "control_claims = matched_df[matched_df['loyalty_program'] == 0]['num_claims']\n",
        "\n",
        "# Perform T-test\n",
        "t_stat, p_value = ttest_ind(treated_claims, control_claims)\n",
        "print(f\"T-test: t-statistic = {t_stat:.3f}, p-value = {p_value:.3f}\")\n",
        "\n",
        "# Visualizing the Effect\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.kdeplot(treated_claims, label='Treated (Loyalty Program)',  fill=True)\n",
        "sns.kdeplot(control_claims, label='Control (No Loyalty Program)', fill=True)\n",
        "plt.xlabel(\"Number of Claims\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Effect of Loyalty Program on Claims\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# If the control group (no loyalty program) has a higher average number of claims, it suggests that the loyalty program reduces claims.\n",
        "# If the distributions overlap completely, it suggests no effect.\n"
      ],
      "metadata": {
        "id": "yJQ_6SRKz6vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute descriptive statistics for treated and control groups\n",
        "treated_mean = np.mean(treated_claims)\n",
        "control_mean = np.mean(control_claims)\n",
        "\n",
        "treated_median = np.median(treated_claims)\n",
        "control_median = np.median(control_claims)\n",
        "\n",
        "treated_variance = np.var(treated_claims)\n",
        "control_variance = np.var(control_claims)\n",
        "\n",
        "# Store results in a DataFrame for easy comparison\n",
        "stats_df = pd.DataFrame({\n",
        "    \"Statistic\": [\"Mean\", \"Median\", \"Variance\"],\n",
        "    \"Treated (Loyalty Program)\": [treated_mean, treated_median, treated_variance],\n",
        "    \"Control (No Loyalty Program)\": [control_mean, control_median, control_variance]\n",
        "})\n",
        "stats_df"
      ],
      "metadata": {
        "id": "GYXo77mWgWhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ATE\n",
        "ATE = treated_claims.mean() - control_claims.mean()\n",
        "ATE\n"
      ],
      "metadata": {
        "id": "UqYE6E_6ZaEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "# Compute Cohen's d (Effect Size) : Effect size measures how meaningful a difference is, beyond just statistical significance.\n",
        "# Small Effect: ùëë<0.2\n",
        "# Medium Effect: 0.2‚â§ùëë<0.5\n",
        "# Large Effect: d‚â•0.8\n",
        "\n",
        "s_pooled = np.sqrt(((len(treated_claims) - 1) * np.var(treated_claims) +\n",
        "                    (len(control_claims) - 1) * np.var(control_claims)) /\n",
        "                   (len(treated_claims) + len(control_claims) - 2))\n",
        "cohens_d = (np.mean(treated_claims) - np.mean(control_claims)) / s_pooled\n",
        "cohens_d\n",
        "#Since Cohen‚Äôs d = -0.4, this means the loyalty program moderately reduces claims.If d were closer to -1, the effect would be stronger.\n"
      ],
      "metadata": {
        "id": "-Zsi7a54lhC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute Required Sample Size (Power Analysis)\n",
        "# Even if a t-test shows statistical significance, we need to ensure our sample size is large enough to detect a real effect.\n",
        "# We use power analysis to determine the minimum sample size required to detect an effect of a given size.\n",
        "# Factors Affecting Sample Size:\n",
        "# Effect size (Cohen‚Äôs d) ‚Üí Larger effect sizes need smaller samples.\n",
        "# Significance Level (ùõº) ‚Üí Typically set to 0.05 (5% chance of false positive).\n",
        "# Power (1‚àíŒ≤) ‚Üí Typically set to 0.8 (80% probability of detecting a true effect).\n",
        "\n",
        "analysis = TTestIndPower()\n",
        "required_sample_size = analysis.solve_power(effect_size=cohens_d, alpha=0.05, power=0.8, ratio=1)\n",
        "required_sample_size"
      ],
      "metadata": {
        "id": "H9dU5Zjnnlgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Z-scores allow us to compare claim behavior across treated and control groups on a common scale.\n",
        "# Step 4: Compute Z-Scores for a few sample claims\n",
        "z_scores_treated = (treated_claims[:10] - np.mean(treated_claims)) / np.std(treated_claims)\n",
        "z_scores_control = (control_claims[:10] - np.mean(control_claims)) / np.std(control_claims)\n",
        "# Display Z-Scores for first 10 customers\n",
        "z_scores_df = pd.DataFrame({\n",
        "    \"Treated Z-Scores\": z_scores_treated,\n",
        "    \"Control Z-Scores\": z_scores_control\n",
        "})\n",
        "z_scores_df"
      ],
      "metadata": {
        "id": "5wGutkjjpK2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results in a DataFrame for easy viewing\n",
        "analysis_results = pd.DataFrame({\n",
        "    \"Statistic\": [\"T-Statistic\",'ATE', \"P-Value\", \"Cohen's d (Effect Size)\", \"Required Sample Size\"],\n",
        "    \"Value\": [t_stat, ATE, p_value, cohens_d, required_sample_size],\n",
        "    \"Interpretation\": [\n",
        "        \"Strong negative effect, indicating fewer claims in the loyalty program\",\n",
        "        \"Negative ATE suggests the loyalty program reduces the number of claims\",\n",
        "        \"Very small p-value confirms statistical significance (rejects null hypothesis)\",\n",
        "        \"Moderate negative effect size, indicating meaningful reduction in claims\",\n",
        "        \"Sample size is sufficient to detect a reliable effect\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "analysis_results"
      ],
      "metadata": {
        "id": "EY4mEvsdpeXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}